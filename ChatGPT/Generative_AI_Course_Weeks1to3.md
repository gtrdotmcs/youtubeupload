
# ğŸ“ Generative AI Course â€” Week 1
**Topic**: Introduction to AI & Generative AI  
**Duration**: ~2 hours  
**Goal**: Build foundational understanding of AI, ML, and what makes generative AI unique.

---

## ğŸ§  PART 1 â€“ Understanding AI, ML, and DL (30â€“40 min)

### ğŸ“˜ Learn the Concepts

#### What is AI?
- Any machine that mimics human intelligence (e.g., rule-based systems).

#### What is ML?
- Algorithms that learn from data instead of being explicitly programmed.

#### What is Deep Learning?
- Subset of ML using neural networks with many layers.

ğŸ“š **Free Resources**:
- [Elements of AI (First 2 chapters)](https://www.elementsofai.com/)
- [YouTube: AI vs ML vs DL by Simplilearn](https://www.youtube.com/watch?v=IoZGSQ07e8g)

---

## ğŸ¨ PART 2 â€“ What is Generative AI? (30â€“40 min)

### ğŸ“˜ Learn the Concepts

#### What is Generative AI?
- Models that **generate** content: text, images, code, music, etc.

#### Common Examples
- Text: ChatGPT, Claude, LLaMA
- Images: DALLÂ·E, Stable Diffusion
- Audio: Jukebox
- Video: Sora (OpenAI)

ğŸ“š **Free Resources**:
- [YouTube: What is Generative AI? by Google](https://www.youtube.com/watch?v=SFJm4BQKnEo)
- [Google AI Glossary](https://ai.google/discover/glossary/)
- [Blog: A Beginnerâ€™s Guide to Generative AI](https://www.analyticsvidhya.com/blog/2023/05/a-beginners-guide-to-generative-ai/)

---

## ğŸ§ª PART 3 â€“ Try a Live Demo (20â€“30 min)

### ğŸ”¬ Try ChatGPT or Gemini

No setup needed, just open:
- [ChatGPT Free](https://chat.openai.com/)
- [Gemini by Google](https://gemini.google.com/app)

Try:
- "Write a short poem about the monsoon."
- "Give me a packing list for a weekend trip."
- "What is generative AI in simple words?"

---

## âœ… Week 1 Progress Checklist

| Task                                                          | Done? âœ… |
|---------------------------------------------------------------|----------|
| Watched AI vs ML vs DL video                                  |          |
| Read first 2 chapters of Elements of AI                       |          |
| Watched Googleâ€™s Generative AI intro video                    |          |
| Tried at least 2 prompts in ChatGPT or Gemini                 |          |
| Can explain the difference between AI, ML, DL, and Generative AI |       |

---
## ğŸ“Œ Coming Next: Week 2 â€” Dive into Language Models (LLMs)
You'll explore:
- What is a language model (LM)?
- What makes a large language model (LLM)?
- How models like GPT-2 and GPT-3 are trained.



# ğŸ¤– Generative AI Course â€” Week 2  
**Topic**: Understanding Language Models & LLMs  
**Duration**: ~2 hours  
**Goal**: Learn how large language models work, what tokens are, and how these models learn to generate language.

---

## ğŸ§  PART 1 â€“ What is a Language Model? (30â€“40 min)

### ğŸ“˜ Learn the Concepts

- A **language model (LM)** predicts the next word in a sequence.
- LLMs like GPT-2, GPT-3, LLaMA are trained on billions of words (tokens).
- Training involves unsupervised learning on large text datasets.

ğŸ“š **Free Resources**:
- [The Illustrated GPT-2 â€“ by Jay Alammar](https://jalammar.github.io/illustrated-gpt2/)
- [What are LLMs? â€“ AssemblyAI Blog](https://www.assemblyai.com/blog/large-language-models/)
- [Short Video: How GPT Works (2 min)](https://www.youtube.com/watch?v=Te5rOTcEalU)

---

## ğŸ’¡ PART 2 â€“ What Are Tokens and Why Do They Matter? (15â€“20 min)

### ğŸ“˜ Learn the Concepts

- Models donâ€™t â€œseeâ€ words â€” they process **tokens** (chunks of text).
- Common tokenizers: Byte-Pair Encoding (BPE), WordPiece, SentencePiece.

Try tokenizing a sentence:

ğŸ”— [OpenAI Tokenizer Tool (Free)](https://platform.openai.com/tokenizer)

Example:
```
Input: "I love learning generative AI."
â†’ Tokens: ['I', ' love', ' learning', ' gener', 'ative', ' AI', '.']
```

---

## ğŸ§ª PART 3 â€“ Watch & Explore Model Internals (30â€“40 min)

### ğŸ§  Understanding Attention & Transformers

ğŸ“š **Resources**:
- [The Illustrated Transformer â€“ by Jay Alammar](https://jalammar.github.io/illustrated-transformer/)
- [How Attention Mechanism Works (Video)](https://www.youtube.com/watch?v=U0s0f995w14)

---

## âœ… Week 2 Progress Checklist

| Task                                                       | Done? âœ… |
|------------------------------------------------------------|----------|
| Read "Illustrated GPT-2"                                   |          |
| Understood what tokens are using tokenizer tool            |          |
| Watched the 2-minute GPT explanation video                 |          |
| Explored the "Illustrated Transformer" visualization       |          |
| Can explain what LLMs are in your own words                |          |

---
## ğŸ“Œ Coming Next: Week 3 â€” Text Generation with GPT-2  
You'll start writing code using the Hugging Face `transformers` library and generate creative text!



# âœï¸ Generative AI Course â€” Week 3  
**Topic**: Text Generation with GPT-2  
**Duration**: ~2 hours  
**Goal**: Use Hugging Face Transformers to generate creative text using GPT-2 and learn how to experiment with model parameters.

---

## ğŸªœ PART 1 â€“ Learn the Concepts (20â€“30 min)

### ğŸ“˜ 1.1 What is GPT-2 and How Does It Work?

GPT-2 is an **autoregressive language model** â€” it generates text word by word, predicting the next word based on previous ones.

ğŸ“š **Free Resources**:
- ğŸ”— [Text Generation Task Summary â€“ Hugging Face](https://huggingface.co/transformers/task_summary.html#text-generation)
- ğŸ”— [Hugging Face Model Card for GPT-2](https://huggingface.co/gpt2)
- ğŸ”— [Hugging Face Transformers GitHub](https://github.com/huggingface/transformers)

**Concepts to focus on:**
- Autoregressive generation
- Token-by-token prediction
- Prompt sensitivity
- Common parameters: `temperature`, `top_p`, `max_length`, `do_sample`

---

## ğŸªœ PART 2 â€“ Hands-On with Google Colab (60â€“70 min)

### ğŸ§ª 2.1 Set Up GPT-2 in Colab

ğŸ”— Open: [Google Colab](https://colab.research.google.com)

Install the library (first-time only):

```python
!pip install transformers
```

Then load GPT-2 and generate basic text:

```python
from transformers import pipeline

generator = pipeline("text-generation", model="gpt2")
prompt = "The future of artificial intelligence is"
result = generator(prompt, max_length=50, num_return_sequences=1)

print(result[0]['generated_text'])
```

---

### ğŸ§ª 2.2 Try Creative Prompts & Tweak Parameters

Experiment with prompts like:
- â€œOnce upon a time, a robot decided toâ€¦â€
- â€œIn the year 3000, Earth was ruled byâ€¦â€
- â€œThe recipe for happiness isâ€¦â€

Try this advanced version:

```python
result = generator(
    prompt,
    max_length=80,
    num_return_sequences=2,
    temperature=0.7,
    top_p=0.9,
    do_sample=True
)
for i, output in enumerate(result):
    print(f"\nResult {i+1}:\n", output["generated_text"])
```

ğŸ“š **Docs to help**:  
ğŸ”— [Text Generation Pipeline Reference](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline)

---

### ğŸ§ª 2.3 Try Other GPT-2 Variants (Optional)

You can test smaller/faster or larger models:
- `"distilgpt2"` â€“ faster, lighter
- `"gpt2-medium"` â€“ more powerful

Update the pipeline like this:

```python
generator = pipeline("text-generation", model="distilgpt2")
```

ğŸ“š Hugging Face Model Hub:  
ğŸ”— [https://huggingface.co/models](https://huggingface.co/models)

---

## âœ… Self Review Questions

1. What happens when you change `temperature` or `top_p`?
2. Whatâ€™s the effect of a vague vs. clear prompt?
3. What model surprised you the most in its output?

---

## ğŸ“‹ Week 3 Progress Checklist

| Task                                                                 | Done? âœ… |
|----------------------------------------------------------------------|----------|
| Read the Hugging Face summary on text generation                     |          |
| Installed `transformers` library in Google Colab                     |          |
| Ran GPT-2 to generate basic text using a prompt                      |          |
| Tried 2+ creative prompts and modified parameters (`max_length`, etc) |          |
| Tested another variant (e.g., `distilgpt2` or `gpt2-medium`)         |          |
| Answered 3 self-review questions                                     |          |

---

## ğŸ“Œ Coming Next: Week 4 â€” Prompt Engineering Basics

In Week 4, youâ€™ll:
- Learn how to write better prompts for more accurate or creative results
- Understand **few-shot prompting** and **role-based prompts**
- Start building your own **prompt library** for different use-cases (e.g., summarizing, writing emails, storytelling)
