# Generative AI — Week 9  
**Topic:** Multimodal AI — Combining Text, Image, Audio, and Video  
**Duration:** 2 hours (Weekend Study)  

---

## 🎯 Learning Objectives
By the end of this week, you will:
- Understand what multimodal AI is and why it’s powerful.
- Learn how to integrate text, images, audio, and video in one workflow.
- Explore open-source multimodal AI frameworks.
- Create a mini-project that blends multiple AI-generated media types.

---

## 📚 Topics Covered
1. **Introduction to Multimodal AI**
   - What “multimodal” means in AI.
   - Examples: ChatGPT with images, video narration from scripts, image-guided video.
   - Real-world applications: content creation, education, marketing, and storytelling.

2. **Core Components**
   - **Text** → For scripting and prompts.
   - **Image** → Generated or reference visuals.
   - **Audio** → Speech synthesis, background music.
   - **Video** → Motion visuals or animation.

3. **Popular Tools & Models**
   - **LLaVA** — Large Language and Vision Assistant.
   - **CLIP** (OpenAI) — Links text and image understanding.
   - **Bark** — Open-source text-to-speech model.
   - **Stable Diffusion + Stable Video Diffusion** — Image and video creation.
   - **Whisper** — OpenAI’s speech-to-text.
   - **Hugging Face Spaces** — Hosts multimodal projects.

4. **Hands-On Practice**
   - Generate a short story (text) → Illustrate with images → Add narration → Combine into a short video.
   - Experiment with image-guided video generation.
   - Try adding sound effects to a generated animation.

---

## 🌐 Open-Source Learning Links

### 📖 Theory
- [Multimodal AI Overview — Microsoft Research](https://www.microsoft.com/en-us/research/project/multimodal-machine-learning/)
- [CLIP: Connecting Text and Images](https://openai.com/research/clip)
- [Introduction to LLaVA](https://llava-vl.github.io/)

### 💻 Practice
- [Bark TTS (Text-to-Speech) on Hugging Face](https://huggingface.co/spaces/suno/bark)
- [Whisper Speech-to-Text](https://github.com/openai/whisper)
- [LLaVA Demo on Hugging Face](https://huggingface.co/spaces/liuhaotian/LLaVA)
- [Stable Diffusion Web UI (Automatic1111)](https://github.com/AUTOMATIC1111/stable-diffusion-webui)
- [ModelScope Text2Video Synthesis](https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis)

---

## 📝 Activity Plan

### Hour 1 — Learn & Explore
- Read about **CLIP** and **LLaVA** to understand how AI can connect text and visuals.
- Explore Bark TTS for voice generation and Whisper for converting speech to text.
- Watch a demo of multimodal AI workflow (text → image → video with audio).

### Hour 2 — Hands-On Mini-Project
- Write a short 3–4 line story (text).
- Generate 3–4 images illustrating each scene (Stable Diffusion).
- Use Bark TTS to create narration.
- Use ModelScope or Stable Video Diffusion to animate the scenes.
- Combine images, audio, and animation into a single short video.

---

## ✅ Progress Checklist
- [ ] Read about multimodal AI basics.
- [ ] Tried at least 2 different multimodal models.
- [ ] Created a mini-project combining text, images, audio, and video.
- [ ] Tested both narration and background music.
- [ ] Saved final video and documented workflow.

---

## 🔜 Coming Next Week  
**Week 10:** Building Your First Generative AI Portfolio Project.  
You’ll put all skills together to create a full AI-generated short film or interactive presentation.

---
