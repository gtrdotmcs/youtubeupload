# Generative AI â€” Week 9  
**Topic:** Multimodal AI â€” Combining Text, Image, Audio, and Video  
**Duration:** 2 hours (Weekend Study)  

---

## ğŸ¯ Learning Objectives
By the end of this week, you will:
- Understand what multimodal AI is and why itâ€™s powerful.
- Learn how to integrate text, images, audio, and video in one workflow.
- Explore open-source multimodal AI frameworks.
- Create a mini-project that blends multiple AI-generated media types.

---

## ğŸ“š Topics Covered
1. **Introduction to Multimodal AI**
   - What â€œmultimodalâ€ means in AI.
   - Examples: ChatGPT with images, video narration from scripts, image-guided video.
   - Real-world applications: content creation, education, marketing, and storytelling.

2. **Core Components**
   - **Text** â†’ For scripting and prompts.
   - **Image** â†’ Generated or reference visuals.
   - **Audio** â†’ Speech synthesis, background music.
   - **Video** â†’ Motion visuals or animation.

3. **Popular Tools & Models**
   - **LLaVA** â€” Large Language and Vision Assistant.
   - **CLIP** (OpenAI) â€” Links text and image understanding.
   - **Bark** â€” Open-source text-to-speech model.
   - **Stable Diffusion + Stable Video Diffusion** â€” Image and video creation.
   - **Whisper** â€” OpenAIâ€™s speech-to-text.
   - **Hugging Face Spaces** â€” Hosts multimodal projects.

4. **Hands-On Practice**
   - Generate a short story (text) â†’ Illustrate with images â†’ Add narration â†’ Combine into a short video.
   - Experiment with image-guided video generation.
   - Try adding sound effects to a generated animation.

---

## ğŸŒ Open-Source Learning Links

### ğŸ“– Theory
- [Multimodal AI Overview â€” Microsoft Research](https://www.microsoft.com/en-us/research/project/multimodal-machine-learning/)
- [CLIP: Connecting Text and Images](https://openai.com/research/clip)
- [Introduction to LLaVA](https://llava-vl.github.io/)

### ğŸ’» Practice
- [Bark TTS (Text-to-Speech) on Hugging Face](https://huggingface.co/spaces/suno/bark)
- [Whisper Speech-to-Text](https://github.com/openai/whisper)
- [LLaVA Demo on Hugging Face](https://huggingface.co/spaces/liuhaotian/LLaVA)
- [Stable Diffusion Web UI (Automatic1111)](https://github.com/AUTOMATIC1111/stable-diffusion-webui)
- [ModelScope Text2Video Synthesis](https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis)

---

## ğŸ“ Activity Plan

### Hour 1 â€” Learn & Explore
- Read about **CLIP** and **LLaVA** to understand how AI can connect text and visuals.
- Explore Bark TTS for voice generation and Whisper for converting speech to text.
- Watch a demo of multimodal AI workflow (text â†’ image â†’ video with audio).

### Hour 2 â€” Hands-On Mini-Project
- Write a short 3â€“4 line story (text).
- Generate 3â€“4 images illustrating each scene (Stable Diffusion).
- Use Bark TTS to create narration.
- Use ModelScope or Stable Video Diffusion to animate the scenes.
- Combine images, audio, and animation into a single short video.

---

## âœ… Progress Checklist
- [ ] Read about multimodal AI basics.
- [ ] Tried at least 2 different multimodal models.
- [ ] Created a mini-project combining text, images, audio, and video.
- [ ] Tested both narration and background music.
- [ ] Saved final video and documented workflow.

---

## ğŸ”œ Coming Next Week  
**Week 10:** Building Your First Generative AI Portfolio Project.  
Youâ€™ll put all skills together to create a full AI-generated short film or interactive presentation.

---
