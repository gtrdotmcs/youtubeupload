# 📚 Generative AI Course — Week 2  
**Topic**: Introduction to Large Language Models (LLMs)  
**Duration**: ~2 hours  
**Goal**: Understand what LLMs are, how they work at a high level, and start generating text using Hugging Face Transformers in Google Colab.

---

## 🪜 PART 1 – Understand LLM Concepts (~40–45 min)

### 📘 1.1 What are LLMs?

Read this beginner-friendly chapter:  
🔗 [Hugging Face NLP Course – Chapter 1: Transformers](https://huggingface.co/learn/nlp-course/chapter1)

📖 Topics covered:
- What is a transformer model?
- Tokenization and embeddings
- Attention mechanism (concept only)

🧠 While reading, reflect on:
- What’s the role of tokenization?
- Why do transformers outperform older models?

---

### 🎥 1.2 Video Supplement (Optional but useful – 10 min)

Watch this animation on how LLMs work:  
🎥 [How GPT Works – Animations](https://www.youtube.com/watch?v=Te5rOTcE7MQ)

---

## 🪜 PART 2 – Generate Text using Hugging Face in Google Colab (~60 min)

### 🧪 2.1 Set Up Colab and Hugging Face Transformers

Open [Google Colab](https://colab.research.google.com) and run:

`python !pip install transformers`

---
 Try your own prompts:

- “The secret to success is…”

- “Once upon a time in a digital world…”

---

### 📘 2.2 Read Documentation (Optional Deep Dive)

Hugging Face Pipeline Reference:

🔗 [Transformers Pipeline Docs](https://huggingface.co/docs/transformers/main_classes/pipelines)

---
✅ Wrap-Up Review
🧠 Self Questions:

1. What is tokenization, and why is it important for LLMs?

2. What is the basic job of a transformer model?

3. What did your GPT-2 model generate that surprised you?
---

### Track

- 📋 Week 2 Progress Checklist
    - [ ] Read Hugging Face Chapter 1 
    - [ ] Watched animation/video on transformers    
	- [ ] Installed Hugging Face Transformers in Google Colab 
	- [ ] Ran a GPT-2 text generation example  
	- [ ] Tried 2–3 custom prompts     
	- [ ] Reflected on 3 self-review questions    
	
---
📌 Coming Next:

<b>Week 3 – Text Generation Project</b>

- Generate poems, summaries, or stories

- Use GPT-2 with different parameters

- Explore model limitations
